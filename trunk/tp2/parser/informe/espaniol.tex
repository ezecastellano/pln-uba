\section{Evaluacion de un chunker en Español}

En esta sección se entrenará y probará la herramienta para ver su reacción ante un idioma más complejo y menos investigado, como el español.

\subsection{Entrenamiento y evaluación de performance}

Para esto utilizamos ChunkerTrainerMe de OpenNLP que nos permite generar un 
binario con nuestro modelo a partir de un archivo de entrenamiento. 

Luego testeamos este binario con el ChunkerEvaluator y los distintos archivos de testeo.

Probamos entrenarlo con A, AA, P y todos. Para esto utilizamos un script en java que nos permitía 
unir todos los archivos de un directorio especificado en uno solo. 

El comando para generar el modelo es:

\texttt{./bin/opennlp ChunkerTrainerME -encoding UTF-8 -lang es -data espaniol/train/X.train -model espaniol/bin/es-chunker.bin}

Mientras que para evaluar los archivos de test:

\texttt{./bin/opennlp ChunkerEvaluator -encoding UTF-8 -data espaniol/test/X.test -model espaniol/bin/es-chunker.bin}

\subsubsection{Entrenado con AA}

\begin{enumerate}

\item Testeado con AA

\begin{itemize}
	\item Precision: 0.8170450806186246
	\item Recall: 0.7892561983471075
	\item F-Measure: 0.8029102667744543
\end{itemize}

\item Testeado con A

\begin{itemize}
	\item Precision: 0.771881461061337
	\item Recall: 0.7296416938110749
	\item F-Measure: 0.7501674480910918
\end{itemize}

\item Testeado con P

\begin{itemize}
	\item Precision: 0.8196829590488771
	\item Recall: 0.7810762614077416
	\item F-Measure: 0.7999140570446366
\end{itemize}

\end{enumerate}

\subsubsection{Entrenado con A}

\begin{enumerate}

\item Testeado con A

\begin{itemize}
	\item Precision: 0.7924784775713638
	\item Recall: 0.7596091205211727
	\item F-Measure: 0.7756957534094688
\end{itemize}

\item Testeado con P

\begin{itemize}
	\item Precision: 0.8016343893428859
	\item Recall: 0.751180111192699
	\item F-Measure: 0.775587566338135
\end{itemize}

\item Testeado con AA

\begin{itemize}
	\item Precision: 0.8012692050768203
	\item Recall: 0.7625556261919898
	\item F-Measure: 0.7814332247557003
\end{itemize}

\end{enumerate}

\subsubsection{Entrenado con P}

\begin{enumerate}

\item Testeado con P

\begin{itemize}
	\item Precision: 0.853230869141682
	\item Recall: 0.8269170250708067
	\item F-Measure: 0.8398678883443426
\end{itemize}

\item Testeado con A

\begin{itemize}
	\item Precision: 0.7801656592791583
	\item Recall: 0.756786102062975
	\item F-Measure: 0.7682980599647266
\end{itemize}

\item Testeado con AA

\begin{itemize}
	\item Precision: 0.8306464745155512
	\item Recall: 0.8107120152574698
	\item F-Measure: 0.8205581919086301
\end{itemize}


\end{enumerate}

\subsubsection{Entrenado con Todos}

\begin{enumerate}

	\item Testeado con P

	\begin{itemize}
		\item Precision: 0.8612941557740088
		\item Recall: 0.8363579146123991
		\item F-Measure: 0.8486428951569984
	\end{itemize}

	\item Testeado con A

	\begin{itemize}
		\item Precision: 0.8256840247131509
		\item Recall: 0.8125950054288816
		\item F-Measure: 0.8190872277552808
	\end{itemize}

	\item Testesado con AA

	\begin{itemize}
		\item Precision: 0.8492927979190376
		\item Recall: 0.8302606484424666
		\item F-Measure: 0.8396688901390341
	\end{itemize}

\end{enumerate}

\subsubsection{Resultados del entrenamiento}

Claramente se puede apreciar que al haber combinado todos los corpus de entrenamiento la performance aumentó de forma general. Habiendo superado en todos los tests a los modelos que estaban basado en uno solo de los archivos.

Respecto a los demás se puede ver que P es el texto que parece ofrecer mejores resultados en los test con todos los entrenamientos y a su vez el que mejor resultados ofrece de los demás textos, y sobre todo de si mismo (sin tener el cuenta el que agrupa todos).


\subsection{Analizando resultados respecto de la construcción algorítmica}


\subsection{Analizando textos en español}

Para analizar algunos textos en español realizamos el proceso de tokenización, postagging y chunking. Para la tokenización decidimos utilizar el modelo en inglés de OpenNLP ya que no consideramos que variase mucho la tokenización respecto del idioma para los textos seleccionados. Para postagging y chunking utilizamos los archivos de entrenamiento brindados por la cátedra. Para poder el archivo de entrenamiento de postagging al formato neceesario por la herramienta utilizamos el Transformador, indicando salida de posttaging sobre el archivo que contenía todos los archivos de entrenamiento. 

Uno de los textos seleccionados fue la descripción de la carrera (http://www.dc.uba.ar/aca/carr/grado/licenciatura/desc) porque nos pareció un texto sin muchas estructuras complejas en cuanto a la tokenización y sin caracteres extraños, pero con varias frases nominales que nos pareció interesante ver como 
eran resueltas. 

El otro texto seleccionado fue una sección del artículo de wikipedia de uno de los temas científicos del momento, el bosón de Higgs (http://es.wikipedia.org/wiki/Bosón\_de\_Higgs). Este texto nos pareció interesante para ver como reaccionaban los modelos utilizando un texto más científico, que contenga algunos porcentajes y unidades que compliquen al un poco más el problema.

Respecto del primer texto los resultados fueron bastante buenos y detectaron algunas frases nominales que no parecían tan triviales como por ejemplo: ``Descripción de la carrera de Licenciatura en Ciencias de la Computación'', ``El plan de estudios de la Licenciatura en Ciencias de la Computación'', ``el título de Analista Universitario de Computación'', entre otras. Aunque también hubo algunos problemas como la inclusión de un paréntesis que abre en una de las frases nominales ``[NP las\_da0fp0 del\_spcms área\_ncfs000 de\_sps00 métodos\_ncmp000 no\_rn numéricos\_aq0mp0 (\_fpa ]'', quedando el que cierra en el principio de otra ``[NP )\_fpt ,\_fc del\_spcms área\_ncfs000 de\_sps00 métodos\_ncmp000 numéricos\_aq0mp0 ] ''. Con las frases verbales también hubo algunos problemas, como por ejemplo ``[VP es\_vsip3s0 ] [NP obligatoria\_ncfs000 salvo\_aq0fs0 ]'' donde dividió ``es obligatoria'' convirtiendo a ``obligatoria salvo'' en una frase nominal sin ningún sentido. También tiene problemas con otras frases verbales como ``se\_p0300000 [VP compone\_vmip3s0 ] de\_sps00'', ``[VP Está\_vmip3s0 ] compuesto\_aq0msp por\_sps00 '' entre otras. Consideramos que con las frases verbales hubo algunos problemas más que con las nominales.

Respecto al otro texto, creemos que si bien los resultados no fueron del todo buenos es interesante rescatar que se detectaron algunas frases nominales complejas como ``[NP su\_dp3cs0 colección\_ncfs000 de\_sps00 datos\_ncmp000 de\_sps00 2012\_default (\_fpa en\_sps00 energías\_ncfp000 de\_sps00 8\_default TeV\_np00000 )\_fpt ]'' donde se respetaron también los paréntesis. Se repitieron errores en las frases verbales como por ejemplo ``[VP haya\_vasp3s0 ] [NP sido\_vsp00sm examinada\_ncfs000 ]'' o ``[VP CL.\_np00000]''. Nos llamó la atención que los números fueran asociados al tag default que pareciera que a la hora de armar el chunk, se amoldara a los tags que se encuentran a su alrededor. Otra cosa que nos despertó curiosidad fue como trataba los valores con unidad, ya que generalmente eran asociados correctamente, pero otras ante una situación similar no sucedía lo mismo, como por ejemplo en este caso donde aparecen dos valores separados por una conjunción, ``[NP 122.1\_default GeV\_np00000 ] ,\_fc y\_cc 129.2\_default [NP GeV\_np00000 ]''.

