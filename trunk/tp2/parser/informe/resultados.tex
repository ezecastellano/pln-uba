\section{Resultados}

\subsection{Postagging Accuracy y las cinco dimensiones de error más frecuentes}

Para calcular esto primero convertimos los archivos de postagging en formato 
OpenNLP a formato BIO y luego utilizamos el programa para comparar postagged
que nos permitía obtener la métrica necesaria y las dimensiones de error más 
frecuentes.

Decidimos que no importaba el en que archivo se produciera la diferencia de 
etiqueta con el otro, es decir no importaba el orden. Por ejemplo que los errores 
NP NNP eran equivalentes a los NNP NP a la hora de sacar la estadística. 

\subsubsection{Genia}

Posttaging Accuracy: 0.866182504333326

\begin{center}
	\begin{tabular}{| c | c | c |}\hline
	Etiqueta & Cantidad & Incidencia\\\hline
	NN NNP   & 2661     & 0.43630103295622236\\
	JJ NN    & 1136     & 0.18626004262993934\\
	JJ VBN   & 197      & 0.0323003771110018\\
	NN CD    & 143      & 0.023446466633874407\\
	NNS VBZ  & 137      & 0.022462698803082472\\\hline
	Subtotal & 4274     &0.7007706181341203\\\hline
	\end{tabular}
\end{center}

\subsubsection{WsjSubset}

Posttaging Accuracy: 0.9715262680203475

\begin{center}
	\begin{tabular}{| c | c | c |}\hline
	Etiqueta & Cantidad & Incidencia\\\hline
	JJ NN    & 187      & 0.13862120088954782\\
	VBN VBD  & 129      & 0.09562638991845812\\
	IN RP    & 95       & 0.07042253521126761\\
	RB IN    & 87       & 0.06449221645663454\\
	NNP NNPS & 77       & 0.05707931801334322\\\hline
	Subtotal & 575      & 0.4262416604892513\\\hline
	\end{tabular}
\end{center}

\subsection{Chunking Precision y Recall para chunks verbales y nominales}

Para calcular estas dos métricas primero pasamos el archivo de chunking generado 
por OpenNLP al formato de BIO y luego utilizando un script hecho en java tomamos 
el archivo original y el procesado por opennlp para generar un archivo que en las 
tres primeras columnas tenía la información original y en la última la obtenida del 
procesamiento para luego poder utilizar el script conlleval.pl .

\subsubsection{Genia}

./conlleval.pl -l < original/gen/genia.compare 

\begin{center}
	\begin{tabular}{| c | c | c | c |}
		\hline
			& Precision &  Recall  & F-Measure \\\hline
		ADJP    &   80.54\% &  70.92\% &  75.42 \\
		ADVP    &   74.36\% &  78.52\% &  76.38 \\
		CC      &    0.00\% &   0.00\% &   0.00 \\
		CD      &    0.00\% &   0.00\% &   0.00 \\
		CONJP   &  100.00\% &  18.03\% &  30.56 \\
		DT      &    0.00\% &   0.00\% &   0.00 \\
		FW      &    0.00\% &   0.00\% &   0.00 \\
		IN      &    0.00\% &   0.00\% &   0.00 \\
		JJ      &    0.00\% &   0.00\% &   0.00 \\
		JJ|RB   &    0.00\% &   0.00\% &   0.00 \\
		LS      &    0.00\% &   0.00\% &   0.00 \\
		LST     &    0.00\% &   0.00\% &   0.00 \\
		NN      &    0.00\% &   0.00\% &   0.00 \\
		NNS     &    0.00\% &   0.00\% &   0.00 \\
		NP      &   86.60\% &  82.57\% &  84.54 \\
		PP      &   92.32\% &  95.41\% &  93.84 \\
		PRT     &  100.00\% &  33.33\% &  50.00 \\
		RB      &    0.00\% &   0.00\% &   0.00 \\
		SBAR    &   92.93\% &  60.88\% &  73.57 \\
		VBN     &    0.00\% &   0.00\% &   0.00 \\
		VP      &   92.39\% &  92.91\% &  92.65 \\
 		DQE &    0.00\% &   0.00\% &   0.00 \\\hline
		Overall &   84.95\% &  86.34\% &  85.64 \\\hline
	\end{tabular}
\end{center}


De estas métricas las que nos interesan son VP (P:92.39\% - R:92.91\%) 
y NP (P:86.60\% - R:82.57\%). 


\subsubsection{WsjSubset}

../../conlleval.pl -l < cmp/wsjsubset.compare 

\begin{center}
	\begin{tabular}{| c | c | c | c |}
		\hline
			& Precision &  Recall  & FF-Measure \\\hline
		''      &    0.00\% &   0.00\% &   0.00 \\
		ADJP    &   79.89\% &  67.12\% &  72.95 \\
		ADVP    &   80.79\% &  77.71\% &  79.22 \\
		CC      &    0.00\% &   0.00\% &   0.00 \\
		CD      &    0.00\% &   0.00\% &   0.00 \\
		CONJP   &   57.14\% &  44.44\% &  50.00 \\
		DT      &    0.00\% &   0.00\% &   0.00 \\
		IN      &    0.00\% &   0.00\% &   0.00 \\
		INTJ    &   50.00\% &  50.00\% &  50.00 \\
		JJ      &    0.00\% &   0.00\% &   0.00 \\
		JJR     &    0.00\% &   0.00\% &   0.00 \\
		LST     &    0.00\% &   0.00\% &   0.00 \\
		MD      &    0.00\% &   0.00\% &   0.00 \\
		NN      &    0.00\% &   0.00\% &   0.00 \\
		NNP     &    0.00\% &   0.00\% &   0.00 \\
		NNPS    &    0.00\% &   0.00\% &   0.00 \\
		NP      &   88.56\% &  90.06\% &  89.30 \\
		PP      &   94.23\% &  97.69\% &  95.93 \\
		PRP     &    0.00\% &   0.00\% &   0.00 \\
		PRT     &   75.00\% &  59.43\% &  66.32 \\
		RB      &    0.00\% &   0.00\% &   0.00 \\
		RBS     &    0.00\% &   0.00\% &   0.00 \\
		SBAR    &   87.84\% &  66.17\% &  75.48 \\
		TO      &    0.00\% &   0.00\% &   0.00 \\
		VB      &    0.00\% &   0.00\% &   0.00 \\
		VBD     &    0.00\% &   0.00\% &   0.00 \\
		VBN     &    0.00\% &   0.00\% &   0.00 \\
		VBZ     &    0.00\% &   0.00\% &   0.00 \\
		VP      &   93.06\% &  92.94\% &  93.00 \\
		``      &    0.00\% &   0.00\% &   0.00 \\\hline
		Overall &   84.88\% &  90.58\% &  87.63 \\\hline
	\end{tabular}
\end{center}

De estas métricas las que nos interesan son VP (P:93.06\% - R:92.94\%) 
y NP (P:88.56\% - R:90.06\%). 

\subsection{Evaluacion de un chunker en español}

En esta sección se entrenará y probará la herramienta para ver su reacción ante un idioma más complejo y menos investigado, como el español.

\subsubsection{Entrenamiento y evaluación de performance}

Para esto utilizamos ChunkerTrainerMe de OpenNLP que nos permite generar un 
binario con nuestro modelo a partir de un archivo de entrenamiento. 

Luego testeamos este binario con el ChunkerEvaluator y los distintos archivos de testeo.

Probamos entrenarlo con A, AA, P y todos. Para esto utilizamos un script en java que nos permitía 
unir todos los archivos de un directorio especificado en uno solo. 

El comando para generar el modelo es:

./bin/opennlp ChunkerTrainerME -encoding UTF-8 -lang es -data espaniol/train/X.train -model espaniol/bin/es-chunker.bin

Mientras que para evaluar los archivos de test:

./bin/opennlp ChunkerEvaluator -encoding UTF-8 -data espaniol/test/X.test -model espaniol/bin/es-chunker.bin

\subsubsubsection{Entrenado con AA}

\begin{enumerate}

\item Testeado con AA

\begin{itemize}
	\item Precision: 0.8170450806186246
	\item Recall: 0.7892561983471075
	\item F-Measure: 0.8029102667744543
\end{itemize}

\item Testeado con A

\begin{itemize}
	\item Precision: 0.771881461061337
	\item Recall: 0.7296416938110749
	\item F-Measure: 0.7501674480910918
\end{itemize}

\item Testeado con P

\begin{itemize}
	\item Precision: 0.8196829590488771
	\item Recall: 0.7810762614077416
	\item F-Measure: 0.7999140570446366
\end{itemize}

\end{enumerate}

\subsubsubsection{Entrenado con A}

\begin{enumerate}

\item Testeado con A

\begin{itemize}
	\item Precision: 0.7924784775713638
	\item Recall: 0.7596091205211727
	\item F-Measure: 0.7756957534094688
\end{itemize}

\item Testeado con P

\begin{itemize}
	\item Precision: 0.8016343893428859
	\item Recall: 0.751180111192699
	\item F-Measure: 0.775587566338135
\end{itemize}

\item Testeado con AA

\begin{itemize}
	\item Precision: 0.8012692050768203
	\item Recall: 0.7625556261919898
	\item F-Measure: 0.7814332247557003
\end{itemize}

\end{enumerate}

\subsubsubsection{Entrenado con P}

\begin{enumerate}

\item Testeado con P

\begin{itemize}
	\item Precision: 0.853230869141682
	\item Recall: 0.8269170250708067
	\item F-Measure: 0.8398678883443426
\end{itemize}

\item Testeado con A

\begin{itemize}
	\item Precision: 0.7801656592791583
	\item Recall: 0.756786102062975
	\item F-Measure: 0.7682980599647266
\end{itemize}

\item Testeado con AA

\begin{itemize}
	\item Precision: 0.8306464745155512
	\item Recall: 0.8107120152574698
	\item F-Measure: 0.8205581919086301
\end{itemize}


\end{enumerate}

\subsubsubsection{Entrenado con Todos}

\begin{enumerate}

	\item Testeado con P

	\begin{itemize}
		\item Precision: 0.8612941557740088
		\item Recall: 0.8363579146123991
		\item F-Measure: 0.8486428951569984
	\end{itemize}

	\item Testeado con A

	\begin{itemize}
		\item Precision: 0.8256840247131509
		\item Recall: 0.8125950054288816
		\item F-Measure: 0.8190872277552808
	\end{itemize}

	\item Testesado con AA

	\begin{itemize}
		\item Precision: 0.8492927979190376
		\item Recall: 0.8302606484424666
		\item F-Measure: 0.8396688901390341
	\end{itemize}

\end{enumerate}

\subsubsubsection{Conclusiones del entrenamiento}

Claramente se puede apreciar que al haber combinado todos los corpus de entrenamiento la performance aumentó de forma general. Habiendo superado en todos los tests a los modelos que estaban basado en uno solo de los archivos.

Respecto a los demás se puede ver que P es el texto que parece ofrecer mejores resultados en los test con todos los entrenamientos y a su vez el que mejor resultados ofrece de los demás textos, y sobre todo de si mismo (sin tener el cuenta el que agrupa todos).


\subsubsection{Analizando resultados respecto de la construcción algorítmica}


\subsubsection{Analizando textos en español}

Para analizar algunos textos en español realizamos el proceso de tokenización, postagging y chunking. Para la tokenización decidimos utilizar el modelo en inglés de OpenNLP ya que no consideramos que variase mucho la tokenización respecto del idioma para los textos seleccionados. Para postagging y chunking utilizamos los archivos de entrenamiento brindados por la cátedra. Para poder el archivo de entrenamiento de postagging al formato neceesario por la herramienta utilizamos el Transformador, indicando salida de posttaging sobre el archivo que contenía todos los archivos de entrenamiento. 

Uno de los textos seleccionados fue la descripción de la carrera (http://www.dc.uba.ar/aca/carr/grado/licenciatura/desc) porque nos pareció un texto sin muchas estructuras complejas en cuanto a la tokenización y sin caracteres extraños, pero con varias frases nominales que nos pareció interesante ver como 
eran resueltas. 

El otro texto seleccionado fue una sección del artículo de wikipedia de uno de los temas científicos del momento, el bosón de Higgs (http://es.wikipedia.org/wiki/Bos\%C3\%B3n\_de_Higgs). Este texto nos pareció interesante para ver como reaccionaban los modelos utilizando un texto más científico, que contenga algunos porcentajes y unidades que compliquen al un poco más el problema.

Respecto del primer texto los resultados fueron bastante buenos y detecto algunas frases nominales que no parecían tan triviales como por ejemplo: "Descripción de la carrera de Licenciatura en Ciencias de la Computación", "El plan de estudios de la Licenciatura en Ciencias de la Computación", "el título de Analista Universitario de Computación", entre otras. Aunque también hubo algunos problemas como la inclusión de un paréntesis que abre en una de las frases nominales "[NP las_da0fp0 del_spcms área_ncfs000 de_sps00 métodos_ncmp000 no_rn numéricos_aq0mp0 (_fpa ]", quedando el que cierra en el principio de otra "[NP )_fpt ,_fc del_spcms área_ncfs000 de_sps00 métodos_ncmp000 numéricos_aq0mp0 ]". Con las frases verbales también hubo algunos problemas, como por ejemplo "[VP es_vsip3s0 ] [NP obligatoria_ncfs000 salvo_aq0fs0 ]" donde dividió "es obligatoria" convirtiendo a "obligatoria salvo" en una frase nominal sin ningún sentido. También tiene problemas con otras frases verbales como "se_p0300000 [VP compone_vmip3s0 ] de_sps00", "[VP Está_vmip3s0 ] compuesto_aq0msp por_sps00" entre otras. Consideramos que con las frases verbales hubo algunos problemas más que con las nominales.

Respecto al otro texto, creemos que si bien los resultados no fueron del todo buenos es interesante rescatar que se detectaron algunas frases nominales complejas como "[NP su_dp3cs0 colección_ncfs000 de_sps00 datos_ncmp000 de_sps00 2012_default (_fpa en_sps00 energías_ncfp000 de_sps00 8_default TeV_np00000 )_fpt ]" donde se respetaron también los paréntesis. Se repitieron errores en las frases verbales como por ejemplo "[VP haya_vasp3s0 ] [NP sido_vsp00sm examinada_ncfs000 ]" o "[VP CL._np00000]". Nos llamó la atención que los números fueran asociados al tag default que pareciera que a la hora de armar el chunk, se amoldara a los tags que se encuentran a su alrededor. Otra cosa que nos despertó curiosidad fue como trataba los valores con unidad, ya que generalmente eran asociados correctamente, pero otras ante una situación similar no sucedía lo mismo, como por ejemplo en este caso donde aparecen dos valores separados por una conjunción, "[NP 122.1_default GeV_np00000 ] ,_fc y_cc 129.2_default [NP GeV_np00000 ]".









